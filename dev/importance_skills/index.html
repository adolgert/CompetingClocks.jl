<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Importance Sampling for Simulation · CompetingClocks.jl Documentation</title><meta name="title" content="Importance Sampling for Simulation · CompetingClocks.jl Documentation"/><meta property="og:title" content="Importance Sampling for Simulation · CompetingClocks.jl Documentation"/><meta property="twitter:title" content="Importance Sampling for Simulation · CompetingClocks.jl Documentation"/><meta name="description" content="Documentation for CompetingClocks.jl Documentation."/><meta property="og:description" content="Documentation for CompetingClocks.jl Documentation."/><meta property="twitter:description" content="Documentation for CompetingClocks.jl Documentation."/><meta property="og:url" content="https://adolgert.github.io/CompetingClocks.jl/importance_skills/"/><meta property="twitter:url" content="https://adolgert.github.io/CompetingClocks.jl/importance_skills/"/><link rel="canonical" href="https://adolgert.github.io/CompetingClocks.jl/importance_skills/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">CompetingClocks.jl Documentation</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Getting Started</span><ul><li><a class="tocitem" href="../install/">Installation</a></li><li><a class="tocitem" href="../quickstart/">Quickstart</a></li><li><a class="tocitem" href="../mainloop/">Sample Main Loop</a></li><li><a class="tocitem" href="../choosing_sampler/">Choosing a Sampler</a></li></ul></li><li><span class="tocitem">User Guide</span><ul><li><a class="tocitem" href="../integration-guide/">Integration Guide</a></li><li><a class="tocitem" href="../low_level_interface/">Low-level Sampler Interface</a></li><li><a class="tocitem" href="../samplers/">Understanding Samplers</a></li><li><a class="tocitem" href="../delayed/">Delayed Clocks</a></li><li><a class="tocitem" href="../guide/">Competing Clocks</a></li><li><a class="tocitem" href="../distributions/">Non-exponential Simulation</a></li><li><a class="tocitem" href="../hierarchical/">Hierarchical Samplers</a></li><li><a class="tocitem" href="../debugging/">Debugging a Simulation that Uses CompetingClocks</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../reliability/">Reliability</a></li><li><a class="tocitem" href="../sir/">SIR Model</a></li><li><a class="tocitem" href="../constant_birth/">Birth-death Process</a></li><li><a class="tocitem" href="../memory/">Transitions with Memory</a></li><li><a class="tocitem" href="../gene_expression/">Gene Expression</a></li></ul></li><li><span class="tocitem">Statistical Methods</span><ul><li><a class="tocitem" href="../commonrandom/">Common Random Numbers</a></li><li class="is-active"><a class="tocitem" href>Importance Sampling for Simulation</a><ul class="internal"><li><a class="tocitem" href="#The-Process"><span>The Process</span></a></li><li><a class="tocitem" href="#Evaluate-proposal-quality"><span>Evaluate proposal quality</span></a></li><li><a class="tocitem" href="#Proposal-Improvement"><span>Proposal Improvement</span></a></li><li><a class="tocitem" href="#Variance-reduction-companions"><span>Variance-reduction companions</span></a></li><li><a class="tocitem" href="#Numerical-and-stability-tools"><span>Numerical and stability tools</span></a></li><li><a class="tocitem" href="#Interpretability-and-sanity-checks"><span>Interpretability and sanity checks</span></a></li><li><a class="tocitem" href="#For-GSMP-specific-models"><span>For GSMP-specific models</span></a></li></ul></li><li><a class="tocitem" href="../hamiltonianmontecarlo/">Hamiltonian Monte Carlo</a></li><li><input class="collapse-toggle" id="menuitem-5-4" type="checkbox"/><label class="tocitem" for="menuitem-5-4"><span class="docs-label">Gen.jl Integration</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../gen/overview/">Gen.jl and CompetingClocks.jl</a></li><li><a class="tocitem" href="../gen/distribution/">CompetingClocks as a Gen Distribution</a></li><li><a class="tocitem" href="../gen/generative_function/">CompetingClocks as a Gen Generative Function</a></li><li><a class="tocitem" href="../gen/observation_likelihood/">Observation Likelihood for Event Data</a></li><li><a class="tocitem" href="../gen/importance_mixture/">Importance Sampling with Mixture Proposals</a></li><li><a class="tocitem" href="../gen/hmc_paths/">HMC over Event Paths with Gen.jl</a></li></ul></li><li><a class="tocitem" href="../gen/turing_dist/">Bayesian Inference with Turing.jl</a></li><li><a class="tocitem" href="../gen/survival_snippet/">Integration with Survival.jl</a></li></ul></li><li><span class="tocitem">API Reference</span><ul><li><a class="tocitem" href="../contextinterface/">Context Interface</a></li><li><a class="tocitem" href="../reference/">Samplers</a></li><li><a class="tocitem" href="../algorithms/">Algorithms</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Statistical Methods</a></li><li class="is-active"><a href>Importance Sampling for Simulation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Importance Sampling for Simulation</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/adolgert/CompetingClocks.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/adolgert/CompetingClocks.jl/blob/main/docs/src/importance_skills.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Importance-Sampling-for-Simulation"><a class="docs-heading-anchor" href="#Importance-Sampling-for-Simulation">Importance Sampling for Simulation</a><a id="Importance-Sampling-for-Simulation-1"></a><a class="docs-heading-anchor-permalink" href="#Importance-Sampling-for-Simulation" title="Permalink"></a></h1><p>Importance sampling is a way to steer a simulation towards simulating the conditions you most care about. The simplest importance sampling is to wait until a simulation gets near a state of interest and then <a href="../reference/#CompetingClocks.split!"><code>CompetingClocks.split!</code></a> the simulation into multiple copies that can better explore that state. The technique can be more powerful with the use of likelihoods, as described below.</p><h2 id="The-Process"><a class="docs-heading-anchor" href="#The-Process">The Process</a><a id="The-Process-1"></a><a class="docs-heading-anchor-permalink" href="#The-Process" title="Permalink"></a></h2><p>When you apply importance sampling in simulation, the workflow feels like this:</p><ol><li><strong>Baseline run:</strong> Simulate under the desired distributions, <span>$p(x)$</span>. If the event is rare, it might not ever happen in your simulations or might happen too few times to get good statistics on it.</li><li><strong>Bias intuition:</strong> Identify which physical parameters or transitions make the event unlikely (e.g., promoter turns off too soon). Modify those rates to define <span>$q(x)$</span>. Try to pick rates that happen once or twice in the simulation, and start with gentle bias.</li><li><strong>Run biased simulations:</strong> Under <span>$q$</span>, the rare event occurs more often.</li><li><strong>Compute path likelihoods:</strong> Using both the <em>biased</em> and <em>true</em> rates, form <span>$w = p(x)/q(x)$</span> for each path.</li><li><strong>Reweight results:</strong> Estimate probabilities or expectations with the weighted average.</li><li><strong>Diagnose:</strong> Check the mean and variance of weights, or compute ESS. If weights vary over many orders of magnitude, adjust <span>$q$</span> to reduce the gap.</li><li><strong>Iterate:</strong> Gradually refine the proposal until ESS stabilizes and probability estimates converge.</li></ol><p>The main problem is that too large of bias on distributions can lead to mathematical underflow in calculation of the weights. Intuitively, a stochastic simulation can have a lot of individual sampled events, and each event&#39;s probability multiplies to get the probability of a path of samples in a trajectory. If those samples are repeatedly biased, they can cause numbers that are too small to represent.</p><p class="math-container">\[w = \frac{L(\lambda_{\text{target}})}{L(\lambda_{\text{proposal}})}
 = \left(\frac{\lambda_{\text{target}}}{\lambda_{\text{proposal}}}\right)^N
e^{-(\lambda_{\text{target}} - \lambda_{\text{proposal}})T}\]</p><p>What you&#39;ll see in practice is that the initial simulation, under <span>$p$</span>, works fine, that a small change in a distribution&#39;s parameters still works fine, and then the importance-weighted estimates fall off a cliff and show values like <span>$10^{-73}$</span>.</p><p>If distributions are scaled, the mean weight can be much less than one. A mean weight that isn&#39;t small is a good sign. We want a sense of the ratio of <span>$w=p/q$</span> the ratio of weights.</p><h2 id="Evaluate-proposal-quality"><a class="docs-heading-anchor" href="#Evaluate-proposal-quality">Evaluate proposal quality</a><a id="Evaluate-proposal-quality-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluate-proposal-quality" title="Permalink"></a></h2><h3 id="Track-how-ESS-scales-with-N"><a class="docs-heading-anchor" href="#Track-how-ESS-scales-with-N">Track how ESS scales with N</a><a id="Track-how-ESS-scales-with-N-1"></a><a class="docs-heading-anchor-permalink" href="#Track-how-ESS-scales-with-N" title="Permalink"></a></h3><pre><code class="language-julia hljs"># Stable log-sum-exp
function logsumexp(x)
    m = maximum(x)
    return m + log(sum(exp.(x .- m)))
end

# Estimate E_q[w] = 1 using the *unshifted* Δ (i.e., don&#39;t subtract max here)
# log mean weight = log( (1/N) * sum(exp(Δ)) )
log_mean_w = logsumexp(Δ) - log(length(Δ))
mean_w_est = exp(log_mean_w)
println(&quot;E_q[w] (should be ~1): &quot;, mean_w_est)

# For self-normalized IS you don’t need this, but it’s a nice sanity check.

println(&quot;mean weight ≈ &quot;, mean_w_est)
w = exp.(Δ)                  # unshifted
# normalize stably:
Z_log = logsumexp(Δ)
wn = exp.(Δ .- Z_log)        # now sum(wn) = 1
ESS_norm = 1 / sum(wn.^2)    # between 1 and N
# Close to (sum(importance)^2) / sum(importance.^2)
println(&quot;ESS (normalized): &quot;, ESS_norm)</code></pre><p>ESS is explained sum of squares. You&#39;re looking for this to be a significant fraction of <span>$N$</span> the number of trajectories.</p><p>If it stops growing linearly with N, your proposal is too far from the target.</p><h3 id="Use-the-coefficient-of-variation-of-weights-to-see-where-variance-explodes."><a class="docs-heading-anchor" href="#Use-the-coefficient-of-variation-of-weights-to-see-where-variance-explodes.">Use the coefficient of variation of weights to see where variance explodes.</a><a id="Use-the-coefficient-of-variation-of-weights-to-see-where-variance-explodes.-1"></a><a class="docs-heading-anchor-permalink" href="#Use-the-coefficient-of-variation-of-weights-to-see-where-variance-explodes." title="Permalink"></a></h3><p class="math-container">\[\text{CV}^2 = \frac{\text{Var}(w)}{E[w]^2} = \frac{N}{\text{ESS}} - 1\]</p><p>Coefficient of variation is a reparametrization of ESS.</p><h3 id="Make-a-weight-histogram"><a class="docs-heading-anchor" href="#Make-a-weight-histogram">Make a weight histogram</a><a id="Make-a-weight-histogram-1"></a><a class="docs-heading-anchor-permalink" href="#Make-a-weight-histogram" title="Permalink"></a></h3><p>Plot log of the weight, <span>$\log_{10}(w)$</span> or <span>$\log(w/\text{mean}(w))$</span>. You want a unimodal, not-too-wide shape. Heavy-tailed distributions indicate you&#39;re close to degeneracy.</p><h2 id="Proposal-Improvement"><a class="docs-heading-anchor" href="#Proposal-Improvement">Proposal Improvement</a><a id="Proposal-Improvement-1"></a><a class="docs-heading-anchor-permalink" href="#Proposal-Improvement" title="Permalink"></a></h2><h3 id="Mixture-proposals"><a class="docs-heading-anchor" href="#Mixture-proposals">Mixture proposals</a><a id="Mixture-proposals-1"></a><a class="docs-heading-anchor-permalink" href="#Mixture-proposals" title="Permalink"></a></h3><p>Instead of one biased model, use a mixture of proposals. This helps if different regions of state space are rare for different reasons. For the gene example, maybe one proposal distribution keeps the promoter on longer and another emphasizes instead reducing degradation of MRNA. You would make one set of distribution parameters for each case and run your simulation where each set of distribution parameters is used a fraction <span>$\alpha_i$</span> of the time.</p><p class="math-container">\[q(x) = \sum_i \alpha_i q_i(x)\]</p><p>Using a mixture of weights is a way to hedge your bets on what about the simulation is causing the outcome to be rare. It does something subtle to the importance calculation because the weight is with respect to the <em>mixture density.</em></p><p class="math-container">\[w_i = \frac{p(x_i)}{\sum_{k=1}^K \alpha_k q_k(x_i)}\]</p><p>In log-space, we would use log-sum-exp.</p><p class="math-container">\[\log w_i = \log p(x_i) - \log\left(\sum_k\alpha_k q_k(x_i)\right) = \log p(x_i) - \text{logsumexp}_k(\log \alpha_k+\log q_k(x_i))\]</p><p>Let&#39;s say we have three proposal distributions from which we sample evenly, so <span>$\alpha=[1/3, 1/3, 1/3]$</span>. We run each simulation where for each enabling of a clock we pass in a vector of four (4) distributions. The first is the proposal distribution we want to use to generate events for this run. The next three are the actual distribution <span>$p$</span> and the other proposal distributions.</p><pre><code class="language-julia hljs">using StatsFuns: logsumexp
log_qmix = logsumexp(log.(α) .+ [log_q1, log_q2, log_q3])
log_weight = log_p - log_qmix
w = exp(log_weight)</code></pre><h3 id="Adaptive-importance-sampling-(AIS)"><a class="docs-heading-anchor" href="#Adaptive-importance-sampling-(AIS)">Adaptive importance sampling (AIS)</a><a id="Adaptive-importance-sampling-(AIS)-1"></a><a class="docs-heading-anchor-permalink" href="#Adaptive-importance-sampling-(AIS)" title="Permalink"></a></h3><p>Iteratively refit your proposal to minimize weight variance. A typical loop:</p><ol><li>Run IS with current proposal.</li><li>Reweight samples to estimate sufficient statistics.</li><li>Fit a new proposal (e.g. update rate multipliers or shift parameters) using the weighted MLE under the target.</li></ol><p>For instance, this could mean adjusting each event&#39;s bias factor until the log-weight variance stops decreasing.</p><h3 id="Cross-entropy-(CE)-method"><a class="docs-heading-anchor" href="#Cross-entropy-(CE)-method">Cross-entropy (CE) method</a><a id="Cross-entropy-(CE)-method-1"></a><a class="docs-heading-anchor-permalink" href="#Cross-entropy-(CE)-method" title="Permalink"></a></h3><p>This structured version of adaptive importance sampling runs the simulation multiple times. You let it tell you the optimal choice of parameters to reduce variance in your estimate of the rare event. One each run, you choose bias parameters <span>$\theta$</span> to minimize the Kullback-Liebler.</p><p class="math-container">\[\text{KL}(p^*||q_\theta)\]</p><p>Here <span>$p^*$</span> is the conditional distribution on the rare event.</p><p>It helps to use common random numbers. Using mixtures is just fine with this method, and you can select among the mixture proposals. As in other cases, the self-normalized estimator is more robust than the unbiased estimator.</p><ol><li>P.-T. de Boer, D. P. Kroese, S. Mannor, and R. Y. Rubinstein. “A Tutorial on the Cross‑Entropy Method.” <em>Annals of Operations Research</em> 134 (2005): 19–67.</li><li>R. Y. Rubinstein. “The Cross‑Entropy Method for Combinatorial and Continuous Optimization.” <em>Methodology and Computing in Applied Probability</em> 1, no. 2 (1999): 127–190.</li><li>R. Y. Rubinstein and D. P. Kroese. <em>The Cross‑Entropy Method: A Unified Approach to Combinatorial Optimization, Monte‑Carlo Simulation and Machine Learning.</em> Springer, 2004.</li></ol><h2 id="Variance-reduction-companions"><a class="docs-heading-anchor" href="#Variance-reduction-companions">Variance-reduction companions</a><a id="Variance-reduction-companions-1"></a><a class="docs-heading-anchor-permalink" href="#Variance-reduction-companions" title="Permalink"></a></h2><h3 id="Self-normalized-Estimator"><a class="docs-heading-anchor" href="#Self-normalized-Estimator">Self-normalized Estimator</a><a id="Self-normalized-Estimator-1"></a><a class="docs-heading-anchor-permalink" href="#Self-normalized-Estimator" title="Permalink"></a></h3><p>Try the self-normalized estimator.</p><p class="math-container">\[\sum_{i=1}^N \mathcal{I}(x_i&gt;0) w_i / \sum_i w_i\]</p><p>The standard estimator.</p><p class="math-container">\[\sum_{i=1}^N \mathcal{I}(x_i&gt;0) w_i / N\]</p><h3 id="Control-variates"><a class="docs-heading-anchor" href="#Control-variates">Control variates</a><a id="Control-variates-1"></a><a class="docs-heading-anchor-permalink" href="#Control-variates" title="Permalink"></a></h3><p>If you know some part of the model&#39;s behavior exactly (like average promoter activity in our example), then you can leverage it to stabilize the estimate of what you don&#39;t know, the rare event probability. The part you know is a <em>correlated statistic,</em> <span>$h(x)$</span>, and what you need to know about it is its expectation, <span>$E_p[h]$</span>. Then you can add this into the total expectation without biasing the final estimate.</p><p class="math-container">\[\hat{\mu}=\sum_i w_i(f(x_i)-c(h(x_i)-E_p[h])).\]</p><p>A good choice of control variate has a strong linear correlation with your outcome, such as producing a lot of proteins in our example. It has a known, or easy-to-calculate, expected value. It doesn&#39;t have wild swings itself which could amplify noise rather than decrease it.</p><p>This technique is often overlooked and can be powerful.</p><h3 id="Stratified-or-quasi-Monte-Carlo-sampling"><a class="docs-heading-anchor" href="#Stratified-or-quasi-Monte-Carlo-sampling">Stratified or quasi-Monte-Carlo sampling</a><a id="Stratified-or-quasi-Monte-Carlo-sampling-1"></a><a class="docs-heading-anchor-permalink" href="#Stratified-or-quasi-Monte-Carlo-sampling" title="Permalink"></a></h3><p>For path-space simulations, you can stratify on states or numbers of events in the chain, ensuring balanced exploration. In Julia, you can use QuasiMonteCarlo.jl to drive RNGs for low-discrepancy trajectories.</p><h3 id="Nested-or-multi-level-Importance-Sampling"><a class="docs-heading-anchor" href="#Nested-or-multi-level-Importance-Sampling">Nested or multi-level Importance Sampling</a><a id="Nested-or-multi-level-Importance-Sampling-1"></a><a class="docs-heading-anchor-permalink" href="#Nested-or-multi-level-Importance-Sampling" title="Permalink"></a></h3><p>For instance, here we are looking for <span>$P(X&gt;1000)$</span>, so split.</p><p class="math-container">\[P(X&gt;1000) = P(X&gt;100)P(X&gt;300|X&gt;100)P(X&gt;600|X&gt;300)\cdots\]</p><p>This splitting / multilevel approach drastically cuts variance for ultra-rare events.</p><h2 id="Numerical-and-stability-tools"><a class="docs-heading-anchor" href="#Numerical-and-stability-tools">Numerical and stability tools</a><a id="Numerical-and-stability-tools-1"></a><a class="docs-heading-anchor-permalink" href="#Numerical-and-stability-tools" title="Permalink"></a></h2><h3 id="Log-sum-exp-normalization"><a class="docs-heading-anchor" href="#Log-sum-exp-normalization">Log-sum-exp normalization</a><a id="Log-sum-exp-normalization-1"></a><a class="docs-heading-anchor-permalink" href="#Log-sum-exp-normalization" title="Permalink"></a></h3><p>Use this for any aggregate quantity that involves exp/log across samples.</p><p>Do the log-space trick. Just like log-sum-exp in machine learning. Underflow is a problem. <span>$\exp(-700)$</span> is small enough to underflow.</p><p class="math-container">\[\frac{e^{\Delta_i}}{\sum_j e^{\Delta_j}} = \frac{e^{\Delta_i}-\text{max}(\Delta)}{\sum_j e^{\Delta_j-\text{max}(\Delta)}}\]</p><p>This makes the probabilities or expectations identical but improves numerical stability.</p><pre><code class="language-julia hljs">Δ = basal - weighted
Δ = Δ .- maximum(Δ)  # stabilize if you ever aggregate multiple paths at once
importance = exp.(Δ)</code></pre><h3 id="Log-domain-accumulation"><a class="docs-heading-anchor" href="#Log-domain-accumulation">Log-domain accumulation</a><a id="Log-domain-accumulation-1"></a><a class="docs-heading-anchor-permalink" href="#Log-domain-accumulation" title="Permalink"></a></h3><p>Keep any running-sums of log-likelihoods with log-add-exp rather than exp-sum-log.</p><h3 id="Outlier-clipping"><a class="docs-heading-anchor" href="#Outlier-clipping">Outlier clipping</a><a id="Outlier-clipping-1"></a><a class="docs-heading-anchor-permalink" href="#Outlier-clipping" title="Permalink"></a></h3><p>In diagnostics (not in the final estimator), cap extermely large weights (say top 0.1%) are recompute ESS. If the estimate hardly changes, your results are stable.</p><h2 id="Interpretability-and-sanity-checks"><a class="docs-heading-anchor" href="#Interpretability-and-sanity-checks">Interpretability and sanity checks</a><a id="Interpretability-and-sanity-checks-1"></a><a class="docs-heading-anchor-permalink" href="#Interpretability-and-sanity-checks" title="Permalink"></a></h2><h3 id="Check-expected-weight"><a class="docs-heading-anchor" href="#Check-expected-weight">Check expected weight</a><a id="Check-expected-weight-1"></a><a class="docs-heading-anchor-permalink" href="#Check-expected-weight" title="Permalink"></a></h3><p>Use log-sum-exp to check that <span>$E_q[w]\approx 1$</span>.</p><h3 id="Check-relative-contribution"><a class="docs-heading-anchor" href="#Check-relative-contribution">Check relative contribution</a><a id="Check-relative-contribution-1"></a><a class="docs-heading-anchor-permalink" href="#Check-relative-contribution" title="Permalink"></a></h3><p>Compute <span>$w_i/\sum_j w_j$</span> for top samples. If a handful contribute over 50% of the total weight, you&#39;re in a weight collapse regime.</p><h3 id="Compare-multiple-bias-directions"><a class="docs-heading-anchor" href="#Compare-multiple-bias-directions">Compare multiple bias directions</a><a id="Compare-multiple-bias-directions-1"></a><a class="docs-heading-anchor-permalink" href="#Compare-multiple-bias-directions" title="Permalink"></a></h3><p>Run several small-bias proposals and look for constistent rare-event probability estimates. If results differ wildly, your current proposals are too aggressive.</p><h2 id="For-GSMP-specific-models"><a class="docs-heading-anchor" href="#For-GSMP-specific-models">For GSMP-specific models</a><a id="For-GSMP-specific-models-1"></a><a class="docs-heading-anchor-permalink" href="#For-GSMP-specific-models" title="Permalink"></a></h2><p>Here each event type has a known distribution, so you can</p><ul><li>bias only a subset of clocks.</li><li>conditionally reweight partial paths as soon as one rare component occurs. This can reduce full-path variance.</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../commonrandom/">« Common Random Numbers</a><a class="docs-footer-nextpage" href="../hamiltonianmontecarlo/">Hamiltonian Monte Carlo »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Friday 28 November 2025 20:19">Friday 28 November 2025</span>. Using Julia version 1.12.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
